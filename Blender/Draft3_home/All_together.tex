\documentclass[twocolumn]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}

\begin{document}
For crystalline materials  with disordered sub-lattices such as the Li ion solid state electrolyte  Li$_2$OHCl it is desirable to calculate from first principles methods the density of energy states $G(E_j)$. With the energy density of states the partition function,
\begin{equation}
\begin{split}
Z = \sum_{i}^{\Omega}e^{\frac{-e_i}{k_B T} }= \sum_{j}^{\Pi}G(E_j)e^{\frac{-E_j}{k_BT}} \;,
\end{split}
\end{equation}
 can be exactly calculated an from it many important properties of interest determined. Properties of interest calculated from the partition function include  free energy, entropy, specific heat, and ensemble averages. One method to solve this problem could be temperature dependent simulations involving  Metropolis algorithm and histogram re-weighting techniques\cite{landau_MC_simulations}.  Another algorithm called the  Wang and Landau algorithm\cite{WL_phys_rev_lett} has been developed which is temperature independent. An issue with these algorithms in use with first principles methods such as density functional theory is the large number of iterations needed which would require a prohibitively long wall time at the current performance power of computers.  In this paper a method is proposed that combines the use of random sets along with the importance sampling method of the Wang and Landau algorithm that is meant to work towards the goal of highly parallel importance sampling algorithms that mesh well with high performance computing architectures. The algorithm developed in this work is referred to as the ``B$_{L}$ENDER" (B$_{L}$end Each New Density Each Round) algorithm. The name ``B$_{L}$ENDER" functions as an acronym and adjective which comes in part because how it blends the ideas of a random set and the Wang and Landau method, and also due to the nature of the algorithm iteratively blending histograms to produce a converged density of states.  The Wang and Landau method does have parallel versions, including  restricting random walkers to specific energy ranges or allowing the walkers to explore the entire space while periodically communicating with each other \cite{MP_Wang_Landau,P_imp_Wang_Landau, Hframe_Wang_Landau}. The ``B$_{L}$ENDER" is natural to parallelize as it is based on a set of random walkers than each can explore the entire energy range. It is hoped that due to the ``B$_{L}$ENDER" being presented in the form of sets that the mathematics of set theory and real analysis can be used to analyze the algorithm and possibly further extended its capabilities from those presented in this work. 

In a previous paper the notion of the master set $\{ \Sigma_i, e_i \}_\Omega $ of the $\Omega$ total configurations and energies was put forth. The master set refers to the actual configurations of the defined system. In practice to calculate the density of energy states $G(E_j)$  may be possible through randomly sampling the configurations space.  In a previous work it was shown that a properly scaled histogram of a sampled set $\{ \Sigma_s, e_s \}_\mathcal{S}$ converges to the exact density of states $G(E_j)$ as the the number of samples $\mathcal{S}$ goes to infinity. The problem with this method is that if $\Omega$ is large, which it is for many problems,  then the computationally effort to achieve convergence is not feasible.  This work tackles this issue produce an algorithm that is highly parallel in terms of the calculations of the energies  but also incorporates importance sampling such as in the Wang and Landau method. 

The ``B$_{L}$ENDER" algorithm proposed in this work  is given as follows. It is noted that the following algorithm is in terms of producing a relative density of states $H(E_j)$.  \\
\begin{equation}
\begin{split}
&1.\hspace{0.125cm} H(E_j)^i ,\hspace{0.15cm}  \{\Sigma_{s},e_s\}_{\mathcal{S}}^i\\
&2. \hspace{0.125cm}\{\Sigma_{s},e_s\}_{\mathcal{S}}^i \rightarrow  \{\Sigma_{s}^{'},e_s^{'}\}_{\mathcal{S}}^i\\
&3. \hspace{0.125cm}H(E_j)^{Ii} = H(E_j)^i + \mathcal{H}(E_j, \{e_{s}^{'}\}_{\mathcal{S}}^i) \\
&4. \hspace{0.125cm} \Sigma_{s}^{i'} \rightarrow \Sigma_{s}^{i+1}, \hspace{0.15cm} P(1, H(e_s)^{Ii}/H(e_s^{'})^{Ii})\\
& \hspace{0.125cm}else  \hspace{0.15cm} \Sigma_{s}^i \rightarrow \Sigma_{s}^{i+1}\\
&5. \hspace{0.125cm} H(E_j)^{i+1} = H(E_j)^{i} + C_o\mathcal{H}(E_j, \{e_s\}_{\mathcal{S}}^{i+1})\frac{H(E_j)^{Ii}}{\sum_j H(E_j)^{Ii} }\\
%&6. \hspace{0.125cm} N = \sum_j G(E_j)^{i+1} \hspace{0.125cm}\\
%& if \hspace{0.125cm} G(E_j)^{i+1}\frac{\Omega}{N}  < 1, \hspace{0.125cm}  G(E_j)^{i+1} = G(E_j)^{i+1} \frac{N}{\Omega}
\end{split}
\end{equation}
Where  $H(E_j)^0 \equiv \mathcal{H}(E_j,\{e_s\}_{\mathcal{S}}^0)$ with $\mathcal{H}(E_j,\{e_s\}_{\mathcal{S}})$ being a histogram function that counts the number of energies $E_j$ in the set $\{e_s\}_{\mathcal{S}}$. In this work $\{\Sigma_{s},e_s\}_{\mathcal{S}}^0$  are a randomly(uniformly) drawn set from the configuration space $\{ \Sigma_i, e_i \}_\Omega $. In the second step  a random change is applied to each element of the sampled set $\{\Sigma_{s},e_s\}_{\mathcal{S}}^i$ to produced a ``perturbed" set $ \{\Sigma_{s}^{'},e_s^{'}\}_{\mathcal{S}}^i$ , for the Ising model this could be randomly flipping a spin. In the third step a histogram of the ``perturbed" set is added to the current estimate of the density of states $H(E_j)^i$ to produce an intermediary density of states $H(E_j)^{Ii}$. In the fourth step a random number is drawn between zero and one for every sampled configuration, if this number is less then the ratio of the density of states $H(e_s)^{Ii}/H(e_s^{'})^{Ii}$ then the perturbed configuration $\Sigma_{s}^{'i}$ goes to $\Sigma_{s}^{i+1}$,  else the unperturbed configuration $\Sigma_{s}^{i}$ goes to $\Sigma_{s}^{i+1}$. In the fifth step a histogram of the updated $\{ e_s \}^{i+1}$ energies is made and added (blended) in to the current density of states $H(E_j)^i$   by multiplying  by a constant(which effects convergence) and the relative probability of each energy $E_j$ in the the intermediary density of states $H(E_j)^{Ii}$. After the algorithm is deemed to be complete it is necessary to re-normalize the iterated relative density of states $H(E_j)^f$ at the final iteration $f$ as follows, 
\begin{equation}
\begin{split}
&1. \hspace{0.125cm} A = \sum_jH(E_j)^f\\
&2. \hspace{0.125cm} G(E_j)\approx H(E_j)^f \frac{\Omega}{A} \;,
\end{split}
\end{equation}
to produce the properly normalized estimated value of $G(E_j)$. 

In this work the algorithm discussed is tested using the 2-d zero field  Ising model.  The configurations $\Sigma_i$ and energies $e_i$ of the 2-d Ising model are inherently defined by the lattice site spin variables $\sigma^i_{k,l}$ and coupling constant $J$. In a previous steady uniform sampling was used with first principles simulations to approximate the partition function for a $2\times 2\times 2$ supercell  of Li$_2$OHCl. In going to just a $3\times 3\times 3$ the value o $\Omega$ would jump from $\sim1e7$ to  $\sim1e26$. So computing the partition function for the $3\times 3\times 3$ system of Li$_2$OHCl is intracable from uniform sampling. In this work a $10X10$ 2d-ising model with $\Omega \approx 1.3e30$ is used as an analogous system to predict the computational effort needed for the B$_L$ENDER algorithm to compute the partition function of a  $3\times 3\times 3$ supercell of Li$_2$OHCl.   A test of the convergence properties with respect to the number of samples $\mathcal{S}$. The first test is a test to show the convergence of the algorithm in terms of the number of samples $\mathcal{S}$ and the number of iterations of the algorithm. To test the accuracy of the simulations the results will be compared to the exact result solved by Beale \cite{Beale_2d_ising}. The accuracy of the simulation will be determined by, 
\begin{equation}
\epsilon(I,o)  = \frac{1}{\Pi} \sum_{j=1}^{\Pi}\frac{|\ln(G_{ex}(E_j))- \ln(G_{bl}(E_j,I,o))|}{\ln(G_{ex}(E_j))}\; . 
\end{equation}

Where $G_{ex}(E_j)$ is the exact density of states, $G_{bl}(E_j,I,o)$ is the density of states at iteration number $I$ from initial conditions and trajectory $o$.
To test the convergence in terms of number of samples $\mathcal{S}$ and number of iterations $I$ for $n=12$.  The simulations where done until iteration average,
\begin{equation}
\mathcal{E}(I) \equiv \langle \hspace{0.1cm} \epsilon(I,o)\hspace{0.1cm}\rangle_{I^{'}} = \frac{1}{I}\int_{0}^{I}\epsilon(I^{'},o)dI^{'} \;,
\end{equation}
 was less then 1$\%$, the corresponding number of iterations was recorded, this was then repeated for multiple simulations. The average value of $I$ over initial configurations and trajectories that produced a specific  value of $\mathcal{E}=c$ equal  was then calculated to give, 
\begin{equation}
 \mathcal{I}(\mathcal{E}=c) \equiv \langle \hspace{0.1cm}  I(\mathcal{E} = c) \hspace{0.1cm}\rangle_{o} \;.
 \end{equation}
 It is noted that the error is only well defined when all of the possible
 energies have been located. Correspondingly we can define an $\mathcal{I}(\mathcal{E}\hspace{0.1cm} != \infty )$ such that what is meant is the average iterations to find all of the energies. 

 From this many important thermodynamic properties can be determined but it may be considered a draw back that ensemble averages of a general order parameter $a_i \equiv a(\Sigma_i)$. Altough not implemented in this work it may be possible to acheive this considering the following equation, 
\begin{equation}
\begin{split}
&\langle a \rangle = \sum_i^{\Omega}a_i \frac{ e^{- \frac{e_i}{K_bT}  }}{Z} = \\
&\sum_j^{\Pi}<a_i>_{j}G(E_j)\frac{e^{-\frac{E_J}{K_bT} }}{Z} \; . 
\end{split} 
\end{equation}
Where $<a_i>_{j}$ is the average of $a_i$ over all configurations with energy $E_j$. So if a running average of the visited values was made during the simulation then it may be possible to approximate ensemble averaged order parameters over the entire temperature range.  
\bibliography{Bib}
\bibliographystyle{unsrt}


\end{document}
