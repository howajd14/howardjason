\documentclass[twocolumn]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}

\begin{document}
In a previous paper the notion of the master set $\{ \Sigma_i, e_i \}_\Omega $ of the $\Omega$ total configurations and energies was put forth. The master set refers to the actual configurations of the defined system. In practice to calculate the density of energy states $G(E_j)$  may be possible through randomly sampling the configurations space.  In a previous work it was shown that a properly scaled histogram of a sampled set $\{ \Sigma_s, e_s \}_\mathcal{S}$ converges to the exact density of states $G(E_j)$ as the the number of samples $\mathcal{S}$ goes to infinity. The problem with this method is that if $\Omega$ is large, which it is for many problems,  then the computationally effort to achieve convergence is not feasible.  This work tackles this issue produce an algorithm that is highly parallel (as is the computation of a random set) but also incorporates importance sampling such as in the Wang and Landau method.

The ``Blender" algorithm proposed in this work  is given as follows. \\
\begin{equation}
\begin{split}
&1.\hspace{0.125cm} G(E_j)^i ,\hspace{0.15cm}  \{\Sigma_{s},e_s\}_{\mathcal{S}}^i\\
&2. \hspace{0.125cm}\{\Sigma_{s},e_s\}_{\mathcal{S}}^i \rightarrow  \{\Sigma_{s}^{'},e_s^{'}\}_{\mathcal{S}}^i\\
&3. \hspace{0.125cm}G(E_j)^{Ii} = G(E_j)^i + \mathcal{H}(E_j, \{e_{s}^{'}\}_{\mathcal{S}}^i) \\
&4. \hspace{0.125cm}\{\Sigma_{s}^{'}\}_{\mathcal{S}}^i \rightarrow \{\Sigma_{s}\}_{\mathcal{S}}^{i+1}, \hspace{0.15cm} P(1, G(e_s)^{Ii}/G(e_s^{'})^{Ii})\\
& \hspace{0.125cm}else  \hspace{0.15cm} \{\Sigma_{s}\}_{\mathcal{S}}^i \rightarrow \{\Sigma_{s}\}_{\mathcal{S}}^{i+1}\\
&5. \hspace{0.125cm} G(E_j)^{i+1} = G(E_j)^{i} + C_o\mathcal{H}(E_j, \{e_s\}_{\mathcal{S}}^{i+1})\frac{G(E_j)^{Ii}}{\sum_j G(E_j)^{Ii} }\\
&6. \hspace{0.125cm} if \hspace{0.125cm} G(E_j)^{i+1}\frac{\Omega}{\sum_j G(E_j)^{i+1}}  < 1, \\
& \hspace{0.125cm} G(E_j)^{i+1} \rightarrow G(E_j)^{i+1} = \frac{\sum_j G(E_j)^{i+1}}{\Omega}
\end{split}
\end{equation}
Where  $G(E_j)^0 \equiv \mathcal{H}(E_j,\{e_s\}_{\mathcal{S}}^0)$ and with $\{\Sigma_{s},e_s\}_{\mathcal{S}}^0$  being a randomly drawn set from the configuration space $\{ \Sigma_i, e_i \}_\Omega $. In the second step  a random change is applied to each element of the sampled set $\{\Sigma_{s},e_s\}_{\mathcal{S}}^i$ to produced a ``perturbed" set $ \{\Sigma_{s}^{'},e_s^{'}\}_{\mathcal{S}}^i$ , for the Ising model this could be randomly flipping a spin. In the third step a histogram of the ``perturbed" set is added to the current estimate of the density of states $G(E_j)^i$ to produce an intermediary density of states $G(E_j)^{Ii}$. In the fourth step a random number is drawn between zero and one for every sampled configuration, if this number is less then the ratio of the density of states $G(e_s)^{Ii}/G(e_s^{'})^{Ii}$ then the perturbed configuration $\Sigma_{s}^{'i}$ goes to $\Sigma_{s}^{i+1}$,  else the unperturbed configuration $\Sigma_{s}^{i}$ goes to $\Sigma_{s}^{i+1}$. In the fifth step a histogram of the updated $\{ e_s \}^{i+1}$ energies is made and added (blended) in to the current density of states $G(E_j)^i$   by multiplying  by a constant(which effects convergence) and the relative probability of each energy $E_j$ in the the intermediary density of states $G(E_j)^{Ii}$.  For a discrete system no element of the properly normalized density of states $G(E_j)$ should  be less than one, so the sixth step corrects for this by normalizing elements that fail this criteria. The procedure of the sixth step helps improve convergence and relies on knowing the the total number of states $\Omega$. In this work the algorithm will be tested on the zero field 2-d ferromagnetic Ising model. 
\end{document}
